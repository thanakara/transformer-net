{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863c3474",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "Making the model ignore padding tokens is trivial using Keras: simply add\n",
    "`mask_zero=True` when creating the Embedding layer. This means that\n",
    "padding tokens (whose ID is 0) will be ignored by all downstream layers.\n",
    "\n",
    "Next, if the layer’s supports_masking attribute is True, then the mask is\n",
    "automatically propagated to the next layer. It keeps propagating this way for\n",
    "as long as the layers have `supports_masking=True`. As an example, a\n",
    "recurrent layer’s supports_mask⁠ ing attribute is True when\n",
    "`return_sequences=True`, but it’s False when `return_sequen⁠ces=False` since\n",
    "there’s no need for a mask anymore in this case. So if you have a model with\n",
    "several recurrent layers with `return_sequences=True`, followed by a recurrent\n",
    "layer with `return_sequences=False`, then the mask will automatically\n",
    "propagate up to the last recurrent layer: that layer will use the mask to ignore\n",
    "masked steps, but it will not propagate the mask any further. Similarly, if you\n",
    "set `mask_zero=True` when creating the Embedding layer in the sentiment\n",
    "analysis model we just built, then the GRU layer will receive and use the\n",
    "mask automatically, but it will not propagate it any further, since\n",
    "return_sequences is not set to True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccca65c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### **TIP**\n",
    "\n",
    "Some layers need to update the mask before propagating it to the next layer: they do so by\n",
    "implementing the `compute_mask()` method, which takes two arguments: the inputs and the\n",
    "previous mask. It then computes the updated mask and returns it. The default\n",
    "implementation of `compute_mask()` just returns the previous mask unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ded212c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "embed_size = 128\n",
       "tf.random.set_seed(42)\n",
       "model = tf.keras.Sequential([\n",
       "    text_vec_layer,\n",
       "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
       "    tf.keras.layers.GRU(128),\n",
       "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
       "])\n",
       "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
       "history = model.fit(train_set, validation_data=valid_set, epochs=5)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "code = \"\"\"\n",
    "embed_size = 128\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)\"\"\"\n",
    "display(Markdown(\"```python\\n{}\\n\".format(code)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac37eb",
   "metadata": {},
   "source": [
    "Using masking layers and automatic mask propagation works best for simple\n",
    "models. It will not always work for more complex models, such as when you\n",
    "need to mix Conv1D layers with recurrent layers. In such cases, you will\n",
    "need to *explicitly compute the mask and pass it* to the appropriate layers,\n",
    "using either the functional API or the subclassing API. For example, the\n",
    "following model is equivalent to the previous model, except it is built using\n",
    "the functional API and handles masking manually. It also adds a bit of\n",
    "dropout since the previous model was overfitting slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22980fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
       "token_ids = text_vec_layer(inputs)\n",
       "mask = tf.math.not_equal(token_ids, 0)\n",
       "Z = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
       "Z = tf.keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\n",
       "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(Z)\n",
       "model = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "token_ids = text_vec_layer(inputs)\n",
    "mask = tf.math.not_equal(token_ids, 0)\n",
    "Z = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
    "Z = tf.keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(Z)\n",
    "model = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\"\"\"\n",
    "display(Markdown(\"```python\\n{}\\n\".format(code)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553acda3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
