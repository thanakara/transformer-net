{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a79c2a",
   "metadata": {},
   "source": [
    "### Stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f521b",
   "metadata": {},
   "source": [
    "Until now, we have only used **stateless** RNNs: at each training iteration the\n",
    "model starts with a hidden state full of zeros, then it updates this state at each\n",
    "time step, and after the last time step, it throws it away as it is not needed\n",
    "anymore. What if we instructed the RNN to preserve this final state after\n",
    "processing a training batch and use it as the initial state for the next training\n",
    "batch? This way the model could learn long-term patterns despite only\n",
    "backpropagating through short sequences. This is called a **stateful** RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a9792",
   "metadata": {},
   "source": [
    "* First, note that a stateful RNN only makes sense if each input sequence in a\n",
    "batch starts exactly where the corresponding sequence in the previous batch\n",
    "left off. So the first thing we need to do to build a stateful RNN is to use\n",
    "sequential and **nonoverlapping** input sequences (rather than the shuffled and\n",
    "overlapping sequences we used to train stateless RNNs). When creating the\n",
    "tf.data.Dataset, we must therefore use `shift=length` (instead of shift=1) when\n",
    "calling the window() method. Moreover, we must **not** call the `shuffle()`\n",
    "method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5324266",
   "metadata": {},
   "source": [
    "* Unfortunately, batching is much harder when preparing a dataset for a\n",
    "stateful RNN than it is for a stateless RNN. Indeed, if we were to call\n",
    "batch(32), then 32 *consecutive* windows would be put in the *same* batch, and\n",
    "the following batch would not continue each of these windows where it left\n",
    "off. The first batch would contain windows 1 to 32 and the second batch\n",
    "would contain windows 33 to 64, so if you consider, say, the first window of\n",
    "each batch (i.e., windows 1 and 33), you can see that they are not\n",
    "consecutive. The simplest solution to this problem is to just use a **batch size\n",
    "of 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435feaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "Distinct characters: \n",
      " !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "filepath = pathlib.Path(\"datasets\") / \"shakespeare.txt\"\n",
    "with open(filepath, \"r\") as f_:\n",
    "    shakespear_txt = f_.read()\n",
    "print(\"\".join(shakespear_txt[:80]))\n",
    "print()\n",
    "print(\"Distinct characters:\", \"\".join(sorted(set(shakespear_txt.lower()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fd67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12], dtype=int64)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    split=\"character\", standardize=\"lower\"\n",
    ")\n",
    "text_vec_layer.adapt([shakespear_txt])\n",
    "encoded = text_vec_layer([shakespear_txt])[0]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f07808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct w/o <PAD> and <UNK>:  39\n",
      "Total dataset size: 1_115_394\n"
     ]
    }
   ],
   "source": [
    "encoded -= 2\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
    "dataset_size = len(encoded)\n",
    "print(\"\\rDistinct w/o <PAD> and <UNK>: \", n_tokens)\n",
    "print(f\"Total dataset size: {dataset_size:_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ece7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset_for_stateful_rnn(sequence, length):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)  # shift = length\n",
    "    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)  # batch size 1\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\n",
    "stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000], length)\n",
    "stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21d30e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[0, 1, 2]])>,\n",
       "  <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[1, 2, 3]])>),\n",
       " (<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[3, 4, 5]])>,\n",
       "  <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[4, 5, 6]])>),\n",
       " (<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[6, 7, 8]])>,\n",
       "  <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[7, 8, 9]])>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple example using to_dataset_for_stateful_rnn:\n",
    "list(to_dataset_for_stateful_rnn(tf.range(10), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d06643",
   "metadata": {},
   "source": [
    "If you'd like to have more than one window per batch, you can use the `to_batched_dataset_for_stateful_rnn()` function instead of `to_dataset_for_stateful_rnn()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848a066",
   "metadata": {},
   "source": [
    "Batching is harder, but it is not impossible. For example, we could chop\n",
    "Shakespeare’s text into 32 texts of equal length, create one dataset of\n",
    "consecutive input sequences for each of them, and finally use\n",
    "`tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))` to\n",
    "create proper consecutive batches, where the $n^{th}$ input sequence in a batch\n",
    "starts off exactly where the $n^{th}$ input sequence ended in the previous batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_non_overlapping_windows(sequence, length):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    return ds.flat_map(lambda window: window.batch(length + 1))\n",
    "\n",
    "\n",
    "def to_batched_dataset_for_stateful_rnn(sequence, length, batch_size=32):\n",
    "    parts = np.array_split(sequence, batch_size)\n",
    "    datasets = tuple(to_non_overlapping_windows(part, length) for part in parts)\n",
    "    ds = tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c270bd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 0,  1,  2],\n",
       "         [10, 11, 12]])>,\n",
       "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 1,  2,  3],\n",
       "         [11, 12, 13]])>),\n",
       " (<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 3,  4,  5],\n",
       "         [13, 14, 15]])>,\n",
       "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 4,  5,  6],\n",
       "         [14, 15, 16]])>),\n",
       " (<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 6,  7,  8],\n",
       "         [16, 17, 18]])>,\n",
       "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "  array([[ 7,  8,  9],\n",
       "         [17, 18, 19]])>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(to_batched_dataset_for_stateful_rnn(tf.range(20), length=3, batch_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e419f9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Code breakdown:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e1df3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, let’s create the stateful RNN. We need to set the `stateful` argument to\n",
    "True when creating each recurrent layer, and because the stateful RNN needs\n",
    "to know the batch size (since it will preserve a state for each input sequence\n",
    "in the batch). Therefore we must set the `batch_input_shape` argument in the\n",
    "first layer. Note that we can leave the second dimension unspecified, since\n",
    "the input sequences could have any length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395b8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (1, None, 16)             624       \n",
      "                                                                 \n",
      " gru (GRU)                   (1, None, 128)            56064     \n",
      "                                                                 \n",
      " dense (Dense)               (1, None, 39)             5031      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61719 (241.09 KB)\n",
      "Trainable params: 61719 (241.09 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=n_tokens, output_dim=16, batch_input_shape=[1, None]\n",
    "        ),  # This is with batch_size = 1\n",
    "        tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n",
    "        tf.keras.layers.Dense(n_tokens, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6cd5ff",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Recall:\n",
    "\n",
    "* The inputs of the Embedding layer will be 2D with shape: [*batch_size*, *window_length*]\n",
    "* The output of the Embedding layer will be 3D with shape: [*batch_size*, *window_length*, *embedding_size*]\n",
    "* The Dense should have *n_tokens* units (same as input_dim of Embedding)\n",
    "* We want to output a probability for each character and should sum up to 1, so we use softmax.\n",
    "* Output of the RNN should be: [1, *window_length*, *n_tokens*]\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385c995",
   "metadata": {},
   "source": [
    "At the end of each epoch, we need to *reset the states* before we go back to the\n",
    "beginning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e64ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431f95f",
   "metadata": {},
   "source": [
    "Compile and train using the ResetStatesCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864cff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
       "history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n",
       "                epochs=10, callbacks=[ResetStatesCallback()])\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "code = \"\"\"\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n",
    "                epochs=10, callbacks=[ResetStatesCallback()])\n",
    "\"\"\"\n",
    "display(Markdown(\"```python\\n{}\\n```\".format(code)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e5e7d6",
   "metadata": {},
   "source": [
    "***Extra***: converting the stateful RNN to a stateless RNN and using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121119ed",
   "metadata": {},
   "source": [
    "To use the model with different batch sizes, we need to create a stateless copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8fac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "        tf.keras.layers.GRU(128, return_sequences=True),\n",
    "        tf.keras.layers.Dense(n_tokens, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21744ea",
   "metadata": {},
   "source": [
    "To set the weights, we first need to build the model (so the weights get created):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbbc05f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d2a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc35683",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential(\n",
    "    [text_vec_layer, tf.keras.layers.Lambda(lambda X: X - 2), stateless_model]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d34f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
